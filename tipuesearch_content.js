var tipuesearch = {"pages":[{"title":"How to Ask for Help","text":"\"Computers are like Old Testament gods; lots of rules, and no mercy.\" — Joseph Campbell Before you ask someone for help, be sure that you have done the following: If there's an error message: read it closely. Though sometimes unclear, all contain information that can help you to understand what's happening. Think about the task you're trying to perform, and as many of the behind-the-scenes steps involved as you can. Consider how the error fits into that stack. Which part of the system is throwing the error: your script, another command, the language, the file system, the network? If the error message isn't immediately helpful, search for it using a search engine. Read the documentation and/or man page. Perhaps what you think you're doing is not actually what the command is doing. If you have considered the above, and both your insight and search engine skills are failing you, then it's likely time to ask someone else for help. When asking someone for help, be sure to provide them with the following information: clearly describe what you're trying to do provide the error message (if any) explain what you've already done to fix/understand this problem A good rule is that the amount of effort you put into understanding/solving a problem is the maximum effort that the other person will invest. To apply this rule to a common situation: if your description is that \"it doesn't work\", you should fully expect no response — as their response-effort precisely matches the effort you've invested into explaining your problem. When you find a solution to your problem, make sure that you are engaged with it — and understand it. The goal is not to just fix the immediate issue, but rather to build a body of knowledge and develop the broader skill of troubleshooting. Computers should be tools which empower you — rather than something inflicted upon you.","tags":"pages","url":"how_to_ask/","loc":"how_to_ask/"},{"title":"Medusa","text":"Medusa [1] is a small computational cluster used for neuroscience research by a hardy cohort of scientists at the Institute of Psychology II at the Otto-von-Guericke University of Magdeburg . Maintained by Alex Waite, Medusa is tailored to the analysis needs of psychology researchers — running Debian Linux with additional research software provided by NeuroDebian When you first use Medusa, you will use one machine: the head node. However, Medusa is a collection of servers, with many dedicated computational nodes. In order to use Medusa to its full potential, you will need to become familiar with our job scheduler, Condor . [1] Medusa, the monster from Greek mythology, had living snakes in place of her hair. The head node of the cluster is called \"medusa\" and each compute node is a \"snake\" (e.g. snake1 , snake2 , etc).","tags":"pages","url":"medusa/","loc":"medusa/"},{"title":"Code of Conduct","text":"Sharing is caring. —Wise Person Medusa is a shared resource, and therefore should be used with awareness and empathy for the work of others. Specific points to pay attention to are: Use Condor for analysis All computational jobs are to be submitted to the HTCondor queue. The head node is meant for interactive use and quick computations, otherwise it negatively affects other people's work. Anything bigger should be submitted as an HTCondor Job, either as an interactive job or as a non-interactive job . HTCondor Job descriptions should be accurate, and users should make an honest intellectual effort to adapt their jobs to the mythical \"ideal job\" . Be mindful of your storage space Treat storage space as if it's a finite resource (pro-tip: it is). Take the time to regularly remove obsolete data and temporary files. Temporary/easily-regeneratable data should be stored in a scratch/ directory. More information is available in the Data Documentation . Report anything strange/faulty If you notice something broken (or even just strange) while working on the cluster, take the time to report it to Alex or Nico. If something isn't right, it likely affects others too.","tags":"medusa","url":"medusa/code_of_conduct/","loc":"medusa/code_of_conduct/"},{"title":"Accessing","text":"Command Line The easiest and most reliable way of connecting to Medusa is via SSH . Connecting is as simple as running the following in your terminal: ssh username@medusa.ovgu.de NOTE: Users with unstable internet connections will likely find tmux to be a helpful tool. Graphical There are multiple methods of connecting to Medusa graphically. X Forwarding Graphical programs run on the remote server can be displayed locally using SSH's X Forwarding . Do note that X Forwarding is very sensitive to latency, so it is only practical to use when on the OvGU campus. ssh -X username@medusa.ovgu.de Sixel Sixel is mostly a toy right now. But it is quite convenient in the few situations where there is support (image and NIfTI previews). VNC VNC is for users who prefer a more familiar desktop experience or need to use graphical programs while off-campus. VNC is a multi-step process and not as easy as straight SSH. First, SSH into Medusa (explained above). Then setup your VNC password. You only need to do this once. The password is stored unencrypted in a text file, so do not use a valuable password (such as for Medusa, email, etc). me@medusa:~$ vncpasswd Password: Verify: Next, start the VNC server. me@medusa:~$ vncserver New 'medusa:9 (me)' desktop is medusa:9 Starting applications specified in /home/me/.vnc/xstartup Log file is /home/me/.vnc/medusa:9.log Note the number 9 in medusa:9 . Yours will likely be different. Take note of your number, as it references your VNC session and will be used later. The VNC server runs until it is terminated by you (or a reboot of Medusa). If you close your client/disconnect, it will continue to run. If you \"log out\" in the session, it will terminate the server. Linux Open a terminal and run the following: vncviewer -via medusa.ovgu.de :9 Note the :9 . Make sure that this is your number from above. This command will first ask that you authenticate to Medusa, and then it will ask for the VNC password you set before. macOS macOS has a VNC client built-in, but it isn't SSH aware. So a SSH tunnel needs to be setup first. ssh -f -L 5909:127.0.0.1:5909 username@medusa.ovgu.de sleep 60 Then you can use VNC. open vnc://127.0.0.1:5909 Note the number \"5909\" in both commands. It should be 5900 + the number noted above. Make sure that it is your number. TODO : write a simple shell script that can be curl -ed into the user's path. Windows Windows lacks a built-in VNC viewer, so you will need TightVNC Java Viewer . Be sure to download the TightVNC Java Viewer ; it is not the first link on their download page. You will also need Java installed in order to run it. Launch the viewer and enter the following information: Remote Host: 127.0.0.1 Port: 5900 + your unique session number (e.g. 5909) Check \"Use SSH Tunneling\" SSH server: medusa.ovgu.de SSH port: 22 SSH User: <your username> Note the port number: \"5909\". It should be 5900 + the number we noted above. Make sure that it is your number. Upon connecting, you will first be asked to authenticate to Medusa; then it will ask for the VNC password you set before. TODO : Move most of this VNC info to tools/VNC . Or avoid the entire mess by migrating to HTTP-based VNC (like Guacamole).","tags":"medusa","url":"medusa/access/","loc":"medusa/access/"},{"title":"HTCondor","text":"This document focuses on how HTCondor is configured and intended to be used on our cluster. To learn about Condor and how to use it, you should read the Tools > HTCondor page. HTCondor jobs come in two versions: interactive and non-interactive. Whenever possible, a non-interactive job should be used for computations. TODO: This page needs some love. The \"Ideal\" Job The \"ideal\" job is [1 CPU × 4 GiB] and runs for 1-3 hours. Of course, not every analysis/step can be broken down into sub-jobs that match this ideal. But experience has shown that, with a little effort, the majority of analysis at IPSY can. Smaller jobs are good: simply, they are more granular and thus better fit (Tetris style) into the available compute resources. Shorter jobs are also preferred; duration directly affects the turnover of jobs and how frequently compute resources become available. If 10,000x 1 hour jobs are submitted, after awhile, a job will be finishing every minute or so (due to normal variations across the cluster). Maintaining liquidity (aka job turnover) is critical for user priority to remain relevant (as discussed in the section Prioritization of Jobs) and ensure the fair-distribution-of and timely-access-to compute resources — rather than merely rewarding those who submit jobs first. 10,000 jobs lasting 1 hour each is far better than 1,000 jobs lasting 10 hours each. Interactive Jobs Any task which takes more than a few minutes or uses a lot of CPU/RAM should not be run from the head node, but as an interactive job. This applies especially to working with datalad , as the underlying git annex calls can be CPU-intensive. To run an interactive job, use the following job submit file . Prioritization of Jobs Condor on Medusa is configured to assess user priority only when jobs are starting. The more compute resources consumed by the user, the more their priority is punished (increased). This \"punishment\" decays back to normal over the course of a day or two. In practice, it works like this: Julie submits 30,000 jobs, each ~1 hour long A day later, Jimbo submits 10 jobs Jimbo's jobs wait in the queue As some of Julie's jobs finish, resources are freed up Both Julie's and Jimbo's jobs compete for the free resources. Jimbo's win because his priority is low (good) and hers is very high (bad). There is also the Priority Factor . Users who are not members of IPSY have a modifier that punishes them even more. This way, in most cases, the jobs of IPSY members will be preferred over those of non-IPSY users. You can check you usage, priority, and priority factor using condor_userprio --allusers . Slots Medusa is configured to allow a diversity of different job sizes, while protecting against large jobs swamping the entire cluster — and also encouraging users to break their analysis into smaller steps. The slots on Medusa are: 16x 1 cpu, 4 GiB ( 4.0 GiB/cpu) 16x 1 cpu, 6 GiB ( 6.0 GiB/cpu) 12x 1 cpu, 5 GiB ( 5.0 GiB/cpu) 6x 10 cpu, 85 GiB ( 8.5 GiB/cpu) 2x 16 cpu, 255 GiB (15.9 GiB/cpu) 1x 48 cpu, 190 GiB ( 3.9 GiB/cpu) 1x 20 cpu, 95 GiB ( 4.7 GiB/cpu) 1x 16 cpu, 415 GiB (25.9 GiB/cpu) 1x 8 cpu, 62 GiB ( 7.7 GiB/cpu) 1x 4 cpu, 18 GiB ( 4.5 GiB/cpu) All slots larger than 1 CPU are partitionable — and thus can be broken into many smaller slots. To illustrate: there are only 44x 1 CPU slots. But if 500x [1 CPU × 4 GiB] jobs are submitted, all of the larger slots are broken up into matching [1 CPU × 4 GiB] slots — resulting in a total of 231 jobs. The reader may have noticed that there are 232 CPUs, and yet only 231 jobs would be scheduled. This is because the [48 CPU × 190 GiB] slot (which has a RAM/CPU ratio < 4 GiB) cannot provide 4 GiB to each CPU; thus, one CPU is left idle. The loss of 1 CPU for [1 CPU × 4 GiB] jobs is negligible. However, as an exercise, the reader is encouraged to determine how much of the cluster would be left idle when submitting [1 CPU × 5 GiB] jobs — and also [2 CPU × 20 GiB]. Matlab By default, Matlab will use all available CPUs. The preferred way to control Matlab is to use the -singleCompthread option. There is also a maxNumCompThreads() function, but was deprecated but now seems to be back from the dead. Any feedback on the efficacy of these function in recent Matlab released (2019a and newer) is most appreciated. NOTE: With the increase in the number of available campus toolbox licenses, it is no longer necessary to restrict Matlab jobs to specific compute nodes. TODO: Discuss Matlab Compiler fMRIPrep By default, fMRIPrep will use all available CPUs and all the RAM it can get. Use the --nthreads and --mem-mb to limit its usage to the requested resources. Intel vs AMD Our cluster's Intel nodes have the fastest single thread performance. If you have very few, single CPU jobs and need them to execute as fast as possible, then restricting your jobs to the nodes with Intel CPUs can be beneficial. The nodes are configured to advertise their CPU vendor, so it is easy to constrain according to CPU type. Add the following to your .submit file. Requirements = CPUVendor == \"INTEL\" Or, to prefer Intel CPUs but not require them Rank = CPUVendor == \"INTEL\"","tags":"medusa","url":"medusa/htcondor/","loc":"medusa/htcondor/"},{"title":"Data","text":"Folder Hierarchy All data in the /home directory is available across the entire cluster. /home/<user_name> This directory is for all of your personal files. /home/data/<project_name> This directory is for data shared across the group/project. /home/<user_name>/scratch or /home/data/<project_name>/scratch This directory is not backed-up and should be used to store interim results which can be easily regenerated. Storing data here helps relieve the burden on backups. /home/data/archive/<project_name> Read-only and heavily compressed (via cool transparent compression mojo), this directory stores data for projects which are completed. If you need to archive data, contact Alex and he'll create a project folder for your data. DataLad When starting new projects, it is highly recommended to use DataLad to ease remote data acquisition and provenance tracking. Backups All data located under the /home directory are snapshot at 05:00 every day — except for any data located in the /home/<user_name>/scratch or /home/data/<project_name>/scratch folders. Snapshots are then transferred to a dedicated backup server located in a different building on the OvGU campus. The backup retention policy is: daily backups kept for 2 weeks weekly backups kept for 6 weeks monthly backups kept for 1 year yearly backups kept for 2.5 years except for /home/data/archive , for which yearly backups are kept \"forever\" If you need to have data recovered from a backup, contact Alex and he will help recover your data. To/From Medusa rsync This is the tool for efficient and reliable transfer. rsync -avh --progress dir_here/ user@medusa.ovgu.de:~/dir_there WinSCP or Cyberduck If you prefer using a GUI, WinSCP (Windows) and Cyberduck (macOS and Windows) are decent SFTP clients. To connect to Medusa: install and launch your client. Enter the information for host ( sftp://medusa.ovgu.de ), user, and password. Click connect. Once connected, you can drag-and-drop files to transfer. TODO: explain SFTP/SSHFS on a per OS basis. Nautilus integration, sshfs on Linux and macOS, and SSHFS-Win on Windows.","tags":"medusa","url":"medusa/data/","loc":"medusa/data/"},{"title":"Software","text":"A wide variety of software is installed across the cluster (AFNI, FSL, FreeSurfer, Octave, Python, R, Matlab, etc). The majority of software is configured to work out-of-the-box, but some software requires additional setup by each user before they can be used. AFNI AFNI ships with commands which are named the same as commands from other packages, so to enable AFNI, run the following: source /etc/afni/afni.sh This needs to be run each time you start a new session on the cluster. If you prefer that it be run automatically, you can add it to your .zshrc file. NOTE: The following commands are known to have conflicts: whirlgif (FSL, wims), gifti_test (FreeSurfer), gifti_tool (FreeSurfer), nifti1_test (Niftilib), nifti_stats (Niftilib), nifti_tool (Niftilib), and whereami (whereami). ANTs ANTs' configuration is setup using something called \"environmental modules.\" First, \"modules\" must be loaded: source /etc/profile.d/modules.sh Then, to load version 2.2.0, run the following: module load ants/2.2.0 Now you can use ANTs. The process needs to be performed each time you start a new session on the cluster. If you'd rather it be done automatically, add the following lines to your .zshrc file. source /etc/profile.d/modules.sh module load ants/2.2.0 NOTE: There are no known namespace conflicts with other commands, so it should be safe to add the above lines to your .zshrc . FreeSurfer FreeSurfer's configuration is setup using something called \"environmental modules.\" First, \"modules\" must be loaded: source /etc/profile.d/modules.sh Then, you can query which versions of FreeSurfer are available: module avail freesurfer Then, to load version 6.0, run the following: module load freesurfer/6.0 Now you can use FreeSurfer. The process needs to be performed each time you start a new session on the cluster. If you'd rather it be done automatically, add the following lines to your .zshrc file. source /etc/profile.d/modules.sh module load freesurfer/6.0 NOTE: The following commands are known to have conflicts: gifti_test (AFNI), gifti_tool (AFNI), and dsh (dsh). FSL FSL ships with commands which are named the same as commands from other packages, so all FSL commands are prepended with fsl5.0- . To configure FSL with defaults and remove the fsl5.0- prefix, run the following: source /etc/fsl/fsl.sh This needs to be run each time you start a new session on the cluster. If you prefer that it be run automatically, you can add it to your .zshrc file. NOTE: The following commands are known to have conflicts: whirlgif (AFNI) and cluster (graphviz). MATLAB Multiple versions of Matlab are installed on the cluster. Because of this, there is no single matlab command. Instead, each version is suffixed with its version number (for example matlab94 ). This suffixing is also applied to mex , mbuild , lmutil , and mcc . You can check the current license usage by running: lmutil96 lmstat -a -c 1984@liclux.urz.uni-magdeburg.de Python Python 2.7 and 3.5 are installed on the cluster — along with a wide variety of modules that are available to import . If you need a python module that is not yet installed and think it is of interest and utility to other cluster users, just ask Alex to deploy it on the cluster. Alternatively, you can install it in a virtualenv . R R 3.3 is installed on the cluster — along with a wide variety of packages from CRAN that are available via the library() command. If you need an R package that is not yet installed (and there is a Debian package for it), just ask Alex to deploy it on the cluster. Alternatively, you can install it from CRAN .","tags":"medusa","url":"medusa/software/","loc":"medusa/software/"},{"title":"Experimental Labs","text":"","tags":"pages","url":"labs/","loc":"labs/"},{"title":"Tools","text":"The greatest of compliments is using a tool in a way never anticipated by the inventor.","tags":"pages","url":"tools/","loc":"tools/"},{"title":"G23-R010: EEG","text":"This lab is operated by the Ullsperger Lab. For questions, problems, or scheduling, contact Christina Becker . Overview The lab has two soundproof cabins with Faraday cages for EEG experiments. Each cabin has its own KVM matrix, which allows for nearly complete flexibility over which computer (EEG recorder, Eye Tracker, Presentation Machine, or researcher supplied Laptop) displays to any screen — and routes both audio and USB. The KVM matrices are also interconnected, so one machine can display to both cabins. Cabin 1 (left) EEG; speakers; eye tracking Cabin 2 (right) EEG; pain stimulation; Current Designs peripherals Software The Presentation Computers run Windows 7 and have been configured in compliance with Brain Products' official guidelines. These machines should never, under any circumstances, be connected to the network. Both machines run Matlab (2012b) and Presentation (multiple versions). EEG Brain Products is the EEG equipment vendor used in this laboratory. Cabin 1 (left) 2x 32-channel BrainAmp MR Plus 1x 16-channel BrainAmp ExG MR Cabin 2 (right) 2x 32-channel BrainAmp DC 1x 16-channel BrainAmp ExG MR The additional Brain Products peripherals available are: 2x StimTrak 2x Photodiode 1x Acoustical Stimulator Adapter 2x GSR (Galvanic Skin Response) sensor Eye Tracker SR Research EyeLink 1000 TODO: discuss camera speed, mount, etc Current Designs A Current Designs 932 unit is connected to the Presentation Computer of the right cabin. The following MR safe peripherals are available: Tethyx Joystick Bimanual Grip Force Pain Stimulation Two Digitimer current stimulators are available, the DS5 and DS7A . In practice, however, only the DS7A is used, as the DS5 is unable to provide enough current to be aversive enough for subjects. The DS7A pulse can be triggered by software, but the intensity cannot be controlled by software. Thus, the initial calibration must be done manually. Speakers Avantone Passive MixCubes were selected because they have a well understood sonic profile (modeled after the well known Auratones) and an overall linear frequency response. They are also shielded, to prevent interference. KVM Matrix The Gefen EXT-DVIKVM-444DL KVM Matrix ( datasheet ; manual ) is the key piece that makes it possible to use all the computers, screens, USB peripherals, and audio devices — in all the needed combinations. It is also a DVI amplifier, enabling reliable signal for the longer cable runs into the EEG cabins. Each matrix can connect to 4 computers (1x DVI; 1x USB; 1x 3.5mm audio) and 4 control stations (1x screen; 1x keyboard; 1x speakers/headphones). Any control station can connect to any computer — and even multiple control stations can be connected to the same computer. To demonstrate how this is useful, the following is a common experiment workflow: EEG Recorder is displayed inside the cabin: as the technician flows the gel into the caps, they need to see which ones have good contact Eye Tracker is displayed inside the cabin: to run the calibration Presentation Machine (or Researcher's Laptop) is displayed inside the cabin: to display the actual experiment Each control station has a Perixx keyboard (Periboard 409) that has 2 additional USB ports. One is used for a mouse; the other can be used by other USB response devices (joystick, etc). These are all routed, via the Gefen matrix, to the appropriate computer. Monitors The colored dots on the monitors match the Gefen matrix remote. Recording Monitor (Green/Input #1) Samsung SyncMaster SA450 The vertical orientation allows viewing many EEG channels simultaneously. Eye Tracking Monitor (Brown/Input #2) Requires a 4:3 ratio monitor Presentation Monitor (Blue/Input #3) Samsung SyncMaster 2233RZ In-Cabin Monitor (Red/Input #4) Samsung SyncMaster 2233RZ The Samsung 2233RZ was specifically chosen because of a paper that measured its timing and found it to be favorable. TODO: link to 2233RZ paper TODO: explain refresh rate vs resolution and matrix Peripherals The following additional peripherals are available: 2x USB Joysticks (1x \"Flightstick Pro\"; 1x \"Fighterstick\") 2x 3-button 9-pin serial response box (custom) 1x 5-button 9-pin serial response box (custom) 1x 25-pin serial foot pedals (custom) TODO: scroll device Zebris TODO: Describe and link: Zebris device","tags":"labs","url":"labs/g23_r010_eeg/","loc":"labs/g23_r010_eeg/"},{"title":"BIDS","text":"The Brain Imaging Data Structure (BIDS) is a set of rules to organize and describe the data from neuroimaging and behavioral experiments. Following this practice makes it easier for scientists to work with each other's data, for both collaborative and reproducible-science purposes. A standard data layout also makes it easier to write analysis tools, and a growing number of tools (see BIDS Apps ) require that input data be BIDS formatted. Do note that BIDS is under active development by the neuroimaging community. BIDS extension proposals (BEPs) are central to the governance for BIDS, and BEPs for other modalities and methods are continuously submitted, discussed, and integrated. Resources These resources will assist you on your BIDS journey: the BIDS specification a BIDS validator Tutorials, wikis, and templates — maintained by the community.","tags":"tools","url":"tools/bids/","loc":"tools/bids/"},{"title":"G24-K010: Behavioral","text":"This lab is operated shared among IPSY. For questions, problems, or scheduling, contact Christina Becker . Overview The lab has three open cabins, a table with a divider for head-to-head experiments, and a desk for those overseeing experiments. Software Open Cabins These 3 computers all run Debian Jessie (8). All machines have Matlab (2012b), PsychoPy, and Psychtoolbox installed. All machines have been configured to match the timing/configuration requirements of both PsychoPy and Psychtoolbox. Because of Matlab's network licensing, each machine is connected to the network. Head-to-Head These 2 computers run Windows 7 and have been configured so all non-essential services are disabled. Both machines have Presentation (multiple versions) installed — and also BrainVision Analyzer 2 (for instruction purposes). Since they are physically close to each other, both screens can easily be connected to the same computer for two-person experiments. Monitors Open Cabins Each cabin is equipped with a Samsung S24C450 Head-to-Head These are non-standard monitors.","tags":"labs","url":"labs/g24_k010_behavioral/","loc":"labs/g24_k010_behavioral/"},{"title":"The Command Line","text":"The shell (sometimes also called a terminal, console, or CLI) is an interactive, text based interface. If you have used Matlab or IPython, then you are already familiar with the basics of a command line interface. While the initial learning curve can be steep, the rewards are well worth it. Command line programs tend to be faster, more flexible, and more scalable than their GUI counterparts. Syntax Commands are case sensitive and follow the syntax of: command [options...] <arguments...> . The options modify the behavior of the program, and are usually preceded by - or -- . For example: ls -l test.txt ls is the command . The option -l tells ls to display more information. test.txt is the argument — the file that ls is listing. Every command has many options (often called \"flags\") that modify their behavior. There are too many to even consider memorizing. Remember the ones you use often, and the rest you will lookup in their documentation or via your favorite search engine. Basic Commands pwd print the name of the folder you're currently in ls -lah <folder> list the contents of a folder, including hidden files ( -a ), and all their information ( -l ); print file sizes in human readable units ( -h ) [1] cd <folder> change to another folder cp <from> <to> copy a file cp -R <from> <to> copy a folder and its contents ( -R ) mv <from> <to> move/rename a file or folder rm <file> delete a file rm -Rv <folder> delete a folder and its contents ( -R ) and list each file as it's being deleted ( -v ) mkdir <folder> create a folder rmdir <folder> delete an empty folder chmod -R g+rwX <folder> give group members ( g+ ) read, write ( rw ), and execute if already present for others ( X ) permissions for a folder and all of its contents ( -R ); see the Section on Permissions for more info. chown -R <username> <folder> change the owner of a folder and all of its contents ( -R ); see the Section on Permissions for more info. chgrp -R <groupname> <folder> change the group of a folder and all of its contents ( -R ); see the Section on Permissions for more info. echo \"text\" print text to the screen man <command_name> show the manual (documentation) for a command Redirection Now that you know some of the basic shell commands, it's time to introduce some core shell concepts. The output that commands print to the screen can also be redirected into a file or used as the input to an another program. > > writes the output of a command to a file. If the file already exists, it will overwrite the contents. For example: echo 'Uhh, what kind of music do you usually have here?' > blues_brothers_reference.txt Or, more practically, the output of a long running search. find /home/data/exppsy -type f -name \"*.fsf\" -print > ffs_these_fsfs.txt >> >> appends the output to a file. If the file doesn't exist, it will create it. echo 'Oh, we got both kinds. We got country *and* western!' >> blues_brothers_reference.txt | | (pipe) redirects the output of a command and uses it as the input for the next command. For example, the following will send the output of echo to sed , which replaces \"stranger\" with \"good looking\". echo 'Well hello there, stranger. <wink>' | sed 's/stranger/good looking/g' More practically, the following command calculates the size of each file and folder in /tmp . The output is then sorted by size. du -sh /tmp/* | sort -h TODO: stdout and stderr TODO: explain clobbering and >| The Prompt When you first login on the command line, you are greeted with \"the prompt\", and it will likely look similar to this: aqw@medusa:~$ This says I am the user aqw on the machine medusa and I am in the folder ~ , which is shorthand for the current user's home folder (in this case /home/aqw ). The $ sign indicates that the prompt is interactive and awaiting user input. [2] In documentation, $ is commonly used as a shorthand for the prompt, and allows the reader to quickly differentiate between lines containing commands vs the output of those commands. For example: $ ls -la wombats.txt -rw-rw---- 1 aqw psyinf 6 Nov 29 10:00 wombats.txt Paths Let's say I want to create a new folder in my home folder, I can run the following command: mkdir /home/aqw/awesome_project And that works. /home/aqw/awesome_project is what is called an absolute path. Absolute paths always start with a / , and define the folder's location with no ambiguity. However, much like in spoken language, using someone's full proper name every time would be exhausting , and thus pronouns are used. This shorthand is called relative paths, because they are defined (wait for it...) relative to your current location on the file system. Relative paths never start with a / . . the current directory .. the parent directory ~ the current user's home directory So, taking the above example again: given that I am in my home folder, the following commands all would create the new folder in the exact same place. mkdir /home/aqw/awesome_project mkdir ~/awesome_project mkdir awesome_project mkdir ./awesome_project To demonstrate this further, consider the following: In my home directory /home/aqw I have added a folder for my current project, awesome_project/ . Let's take a look at how this folder is organized: └── home └── aqw └── awesome_project ├── aligned ├── code └── sub-01 └── bold3T └── sub-02 └── bold3T ├── ... └── sub-xx └── bold3T └── structural └── sub-01 └── anat └── sub-02 └── anat ├── ... └── sub-xx └── anat Now let's say I want to change from my home directory /home/aqw into the code/ folder of the project. I could use absolute paths: cd /home/aqw/awesome_project/aligned/code But that is a bit wordy. It is much easier with a relative path: cd awesome_project/aligned/code Relative to my starting location ( /home/aqw ), I navigated into the subfolders. I can change back to my home directory also with a relative path: cd ../../../ The first ../ takes me from code/ to its parent aligned/ , the second ../ to awesome_project/ , and the last ../ back to my home directory aqw/ . However, since I want to go back to my home folder, it's much faster to run: cd ~ Globbing Most modern shells have powerful pattern matching abilities (often called globbing) that allows you to match the names of multiple files and/or directories. This is especially useful when running a command on many files at once. When globbing, the shell compares the pattern to files on the file system and expands the term to all matching file names. The most basic pattern is * , which matches any number of any character(s). For example, the following will list all files in the current directing ending in .txt : ls *.txt Or, lets you move a bunch of .jpg files into a folder: mv -v *.jpg annoying_instagram_food_pics/ Globbing can also nest through directories. For example, assuming a typical folder structure for subject data, you can list every subject's functional .nii.gz files for run 1: ls sub-*/func/*_run-1_*.nii.gz You can read about more about Pattern Matching in Bash's Docs . Permissions Every file and folder has permissions which determine which users are allowed to read, write, and execute it. $ ls -la wombats.txt -rw-rw---- 1 aqw psyinf 6 Nov 29 10:00 wombats.txt The -rw-rw---- provides all the information about this file's permissions. The left-most - indicates whether it's a file, a folder ( d ), a symlink ( l ), etc. The rest are three tuplets of --- . The first tuplet is for the user, the second tuplet is for the group, the last tuplet is for all other users. The above example shows that both the user ( aqw ) and the group ( psyinf ) have read and write permissions ( rw- ) to wombats.txt . All other users on the system have no permissions ( --- ). Let's say I don't want others in the psyinf group to have permission to write to wombats.txt anymore. $ chmod g-w wombats.txt $ ls -lah wombats.txt -rw-r----- 1 aqw psyinf 6 Nov 29 10:00 wombats.txt TODO: explain chmod 640 vs chmod g-w TODO: discuss (and show how to set UMASK) TODO: discuss user-private groups, sticky bit TODO: point to a more exhaustive explanation and/or man page Useful Commands ssh <username>@<servername> log into an interactive shell on another machine passwd change your password rsync -avh --progress from_folder/ <user>@<server>:/destination/folder sync/copy from a local folder to a folder on a remote server via SSH. Will preserve all permissions, checksum all transfers, and display its progress. grep -Ri <term> <folder> case-insensitive search for a term for all files under a folder htop overview of computer's CPU/RAM and running processes pip install --user <python_pip_package> install Python packages into your home folder sed -i \"s/oops/fixed/g\" <file> replace all occurrences of 'oops' with 'fixed' in a file wget <link> download a file find <folder> -type d -exec chmod g+s {} \\; find all folders underneath a directory and apply the \"sticky bit\" to them; see the Section on Permissions for more info. du -sh <folder> print how much disk space a folder uses cat <file> print the contents of a file to the screen head -n 20 <file> show the first 20 lines of a file tail -n 10 <file> show the last 10 lines of a file tail -f <file> print the last 10 lines of a file, and continue to print any new lines added to the file (useful for following log files) less <file> print the content of a file to the screen, one screen at a time. While cat will print the whole file, regardless of whether it fits the terminal size, less will print the first lines of a file and let you navigate forward and backward ln -s <target> <link_name> create a symlink (a shortcut) TODO: sudo TODO: unzip/tar/gzip TODO: sshfs (different section/page?) Piping Fun du -sh ./* | sort -h calculate the size of each of the files and folders that are children of the current folder, and then sort by size find ./ -mmin -60 | wc -l find all files under the current directory that have been modified in the last 60 minutes, and then count how many are found ls -lah ~/ | less list all files in your home folder and display them one page at a time Text Editors Text editors are a crucial tool for any Linux user. You will often find the need for one, whether it is to quickly edit a file or write a collection of analysis scripts. Religious wars have been fought over which is \"the best\" editor. From the smoldering ashes, this is the breakdown: nano Easy to use; medium features. If you're unsure of what to use, start with this. vim Powerful and light; lots of features and many plugins; steep learning curve. Two resources to help get the most out of vim are the vimtutor program (already installed on on the cluster) and vimcasts.org . emacs Powerful; tons of features; written in Lisp; huge ecosystem; advanced learning curve. Shells Whenever you use the command line on a Unix-based system, you do that in a command-line interpreter that is referred to as a shell . The shell is used to start commands and display the output of those commands. It also comes with its own primitive (yet surprisingly powerful) scripting language. [3] Many shells exist, though most belong to a family of shells called \"Bourne Shells\" that descend from the original sh . This is relevant, because they share (mostly) a common syntax. Common shells are: bash The bourne-again shell ( bash ) is the default shell on many *nix systems (most Linux distros, MacOS). zsh The Z shell comes with many useful features, such as: shared history across running shells, substring search for history, smarter tab-completion, spelling correction, and better theming. tcsh The C shell (both csh and tcsh ) is deprecated and should not be used. Some legacy systems use it, but is strongly encouraged to switch to either zsh or bash . Comparatively, C shell has a limited feature set. But most importantly, it is not a member of the Bourne family of shells, and thus uses a different syntax. To determine what shell you're in, run the following: echo $SHELL TODO: history (up and searching), zsh history substring search Tab Completion One of the best features ever invented is tab completion . Imagine your spirit animal. Now imagine that animal sitting on your shoulder and shouting \"TAB!\" every time you've typed the first 3 letters of a word. Listen to your spirit animal's voice. Tab completion autocompletes commands and paths when you press the Tab key. If there are multiple matching options, pressing Tab twice will list them. The greatest advantage of tab completion is not increased speed (though that is a nice benefit) but rather the near elimination of typos — and the resulting reduction of cognitive load. You can actually focus on the task you're working on, rather than your typing. For an example of tab-completion with paths, consider the following directory structure: ├── Desktop ├── Documents │ ├── my_awesome_project │ └── my_comics │ └── xkcd │ │ └── is_it_worth_the_time.png ├── Downloads You're in your home directory, and you want to navigate to your xkcd comic selection in Documents/my_comics/xkcd . Instead of typing the full path error-free, you can press Tab after the first few letters. If it is unambiguous, such as cd Doc <Tab> , it will expand to cd Documents . If there are multiple matching options, such as cd Do , you will be prompted for more letters. Pressing Tab again will list the matching options ( Documents and Downloads in this case). A visual example of tab-completion in action: There are more sophisticated completion scripts, but they are not always enabled by default. For example, git add -p <TAB> will list only modified files. zsh can expand multiple levels at a time: cd d/m/x <TAB> will complete to cd Documents/my_comics/xkcd . [1] By default, file sizes are printed in Bytes. The -h flag changes this to units sane for human consumption. For example: 137216 would instead be listed as 134K. And for those brains rioting right now, remember, computers are binary, so 1K is 1024 bytes (2 10 ), not 1000 (10 3 ). [2] The # symbol is commonly used to indicate a prompt with elevated permissions (such as the root user). [3] As always, the man page ( man bash ) is a great reference. But if you're interested in acquiring a deep understanding of shell, then I highly recommend \"Beginning Portable Shell Scripting\" by Peter Seebach.","tags":"tools","url":"tools/cli/","loc":"tools/cli/"},{"title":"G24-K012: Eye Tracker","text":"This lab is operated by the Pollmann Lab. For questions, problems, or scheduling, contact Mikaella Sarrou . NOTE: This lab is due to be overhauled in 2018. Thus this documentation is limited — at best. Overview The lab is used primarily for behavioral and eye tracking experiments. TODO: mention back projected screen Software There are two Presentation Computers. One is a legacy, Windows XP machine (with \"some version\" of Matlab installed). The other runs a semi-standard Debian Jessie (8) setup with Matlab 2012b, PsychoPy, and Psychtoolbox installed. It has been configured to match the timing/configuration requirements of both PsychoPy and Psychtoolbox. There is no KVM-Matrix, so switching between the two requires that all wires be disconnected and reconnected to the desired computer. Eye Tracker SR Research EyeLink 1000 TODO: discuss camera speed, mount, etc Audio Speakers (of unknown manufacture) are available. Also available are Beyerdynamic DT-770 M headphones. Back Projected Screen TODO: Describe and link: big, back projected screen. TODO: Describe and link: projector Monitors The Presentation Monitor is a BenQ XL2410T . Peripherals The following peripherals are available: 5-button, 25-pin serial VPixx technologies ResponsePixx VPX-ACC-3000 Handheld","tags":"labs","url":"labs/g24_k012_eyetracker/","loc":"labs/g24_k012_eyetracker/"},{"title":"DAGMan","text":"DAGMan is a HTCondor tool that allows multiple jobs to be organized in workflows. A DAGMan workflow automatically submits jobs in a particular order, such that certain jobs need to complete before others start running. Once you are familiar with how to create, submit, and monitor HTCondor jobs , creating DAGMan workflows is relatively easy. The official documentation describes comprehensively the overall structure and available scripting of the dag-file. A simple dag file consists of a list of nodes (which are jobs plus optional pre- and post-processing scripts). In addition, their relationship can be specified via PARENT JobName CHILD JobName structures. Example A simple use-case for DAGMan is when wanting to run a set of jobs one after another, without having to submit each job manually once the previous one finishes (for example, when importing dicoms using datalad hirni-import-dcm or preprocessing multiple subjects using fmriPrep in sequence - both of these do not run well in parallel at the time of writing). To achieve this two files are needed: a submit-file (e.g. my_job.submit ) and a dag-file (e.g. my_workflow.dag ). The submit-file is a regular HTCondor submit file, but in addition, it can have special variables which will be set via the dag file on submission: #### The submit file - my_job.submit # The environment universe = vanilla getenv = True request_cpus = $(req_cpu) request_memory = $(req_mem) # Execution initial_dir = $ENV(HOME) executable = $ENV(HOME)/my_nature_worthy_analysis.sh # Job 1 # NOTE: arguments 2 and 3 are request_cpus and request_memory, respectively arguments = \"$(subject) $(req_mem) $(req_mem)\" log = $ENV(HOME)/logs/fortune_$(Cluster).$(Process).log output = $ENV(HOME)/logs/fortune_$(Cluster).$(Process).out error = $ENV(HOME)/logs/fortune_$(Cluster).$(Process).err Queue In the above script, three additional, non-standard variables are included: req_cpu , req_mem , and subject . The first two are used to dynamically specify how many resources are needed for the job, as well as to be passed on to the analysis-script via the arguments of the job. The variable subject is only passed on to the analysis script. In the associated dag-file, these variables can be set for all nodes (using VARS ALL_NODES .. ) or for only specific nodes ( VARS JobName .. ). The example below shows how to set requirements for all jobs; the node names (a.k.a JobName 001 , 002 , and 003 ) are used to dynamically set the subject numbers. #### my_workflow.dag JOB 001 my_job.submit JOB 002 my_job.submit JOB 003 my_job.submit VARS ALL_NODES req_mem=\"1000\" VARS ALL_NODES req_cpu=\"2\" VARS ALL_NODES subject=\"$(JOB)\" CATEGORY ALL_NODES DummyCategory MAXJOBS DummyCategory 1 Finally, it is possible to limit the number of concurrently running jobs for each category of job. In this example, only one category ( DummyCategory ) is created and its MAXJOBS value is set to 1. Submitting the dag-file using condor_submit_dag my_workflow.dag will add the workflow to condor's queue and execute all three jobs one after another, making sure that only one of them is running at a given time: -- Schedd: medusa.local : <10.0.0.100:9618?... @ 02/11/20 20:58:05 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS pvavra my_workflow.dag+1898 2/11 19:57 _ 1 _ 3 1898912.0 2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended","tags":"tools","url":"tools/dagman/","loc":"tools/dagman/"},{"title":"DataLad","text":"DataLad is a data management tool to share, search, obtain, extend, and version data. Beware that the underlying git annex calls (e.g. when calling datalad save ) can be CPU-intensive if a dataset contains many files (e.g. a typical fMRI dataset). Thus, it is important to work on such datasets via an interactive job . Resources The DataLad Handbook is the most comprehensive effort to document DataLad. However, it is still very much a work in progress as currently the content changes almost daily. DataLad Docs Common Workflows TODO: Provide examples of a common IPSY workflows.","tags":"tools","url":"tools/datalad/","loc":"tools/datalad/"},{"title":"G24-K013: Soundproof","text":"This lab is operated by the Pollmann Lab. For questions, problems, or scheduling, contact Mikaella Sarrou . Overview The lab has one soundproof lab for eye tracking experiments. It is also equipped with a KVM matrix, which allows for nearly complete flexibility over which computer (EEG recorder, Eye Tracker, Presentation Machine, or researcher supplied Laptop) displays to any screen — and routes both audio and USB. Software The Presentation Computer runs Debian Jessie (8) with Matlab (2012b), PsychoPy, and Psychtoolbox installed. It has been configured to match the timing/configuration requirements of both PsychoPy and Psychtoolbox. Eye Tracker SR Research EyeLink 1000 TODO: discuss camera speed, mount, etc Communication with the eye tracker is done via the yellow Ethernet cable. The Presentation computer is already setup. If you need to communicate using your laptop, use the following settings to configure your NIC: Address: 100.1.1.2 Netmask: 255.255.255.0 Gateway: 100.1.1.1 Speakers A portable, USB-rechargable, battery operated speaker is available. KVM Matrix The Gefen EXT-DVIKVM-444DL KVM Matrix ( datasheet ; manual ) is the key piece that makes it possible to use all the computers, screens, USB peripherals, and audio devices — in all the needed combinations. It is also a DVI amplifier, enabling reliable signal for the longer cable runs into the EEG cabin. The matrix can connect to 4 computers (1x DVI; 1x USB; 1x 3.5mm audio) and 4 control stations (1x screen; 1x keyboard; 1x speakers/headphones). Any control station can connect to any computer — and even multiple control stations can be connected to the same computer. To demonstrate how this is useful, the following is a common experiment workflow: Eye Tracker is displayed inside the cabin: to run the calibration Presentation Machine (or Researcher's Laptop) is displayed inside the cabin: to display the actual experiment Each control station has a Perixx keyboard (Periboard 409) that has 2 additional USB ports. One is used for a mouse; the other can be used by other USB response devices (joystick, etc). These are all routed, via the Matrix, to the appropriate computer. Monitors The colored dots on the monitors match the Gefen matrix remote. Presentation Monitor (Green/Input #1) iiyama G-MASTER GB2488HSU-B2 Eye Tracking Monitor (Brown/Input #2) Requires a 4:3 ratio monitor In-Cabin Monitor (Red/Input #4) iiyama ProLite GB2488HSU-B1 The In-Cabin Monitor is wall mounted with an adjustable arm and swivel. This allows the monitor height and depth to be adjusted for experiments which require different visual-fields. A laser distance meter and level is available (on the shelf near the doorway) to verify that the screen is positioned correctly. TODO: explain refresh rate vs resolution and matrix Peripherals The following peripherals are available: 8-button, USB Cedrus RB-830 Response Pad 5-button, 25-pin serial Psychology Software Tools 200A Response Box 6-button, 25-pin serial Cedrus RB-600 Response Box","tags":"labs","url":"labs/g24_k013_soundproof/","loc":"labs/g24_k013_soundproof/"},{"title":"Git","text":"Git enables you to track the changes made to files over time — specifically: what changed, by whom, when, and why. It also gives you the capability to revert files back to a previous state. Over time, as your project evolves, you can edit your files with confidence knowing that at any point you can look back and recover a previous version. Install Debian/Ubuntu sudo apt install git macOS Download the installer at: https://git-scm.com/download/mac Windows Download the installer at: https://git-scm.com/download/win Configure Once Git is installed, configure it with your name and email address. This lets Git know who you are so it can associate you with the commits you make. git config --global user.name \"Wiggly McWidgit\" git config --global user.email wiggity-wiggity-wack@example.com Repository Anatomy A git repository is just a folder that git is tracking the changes in. All of the behind-the-scenes information and metadata that allows git to perform its magic is stored in the .git/ folder in top folder of your project. There are two types of files for git: tracked and untracked files. Tracked files are files that git is aware of (via git add ). Only tracked files are considered part of the repository, and thus only tracked files have the special powers-of-git bestowed upon them: history, commit messages, and ability to share/merge history through git push and git pull . Untracked files are just the normal file-experience: no history, no reverting, no easy sharing, etc. Git has three different states for tracked files: modified The file has been modified. The changes are not (yet) part of your project's history. staged Modified content which is staged to be committed , but is not yet committed. The staging step is so that you can review whether the changes should be committed and to be able to stage multiple changes/files for a commit. Content is staged using git add / git add -p . committed The content is stored in your history. Content is committed using the git commit command. Basic Commands git init Creates a repository in this folder. Note, no files are tracked ( git add -ed yet. git clone <url> | <user@server:/path/to/repo.git> Makes a full copy of an existing git repository — all files, folders, changes, history, etc. git status Lists which files are in which state — if there have been changes made, new files added or deleted, etc. git add <file> To begin tracking a new file. Once you run git add , your file will be tracked and staged to be committed. git add -p Review the changes you've made to tracked files, and choose which changes will be staged . git commit Commits all the staged changes (see git add ). It will prompt you for a commit message , which should be a terse but descriptive note about the changes contained in the commit. These commit messages are your project's history. git rm <file> Deletes the file, and stages the deletion. Note that the file and its contents remain in the project history, and can be recovered. git mv <file-from> <file-to> Moves/renames a file and stages the rename. git log Lists your commit history. It's not as user-friendly or easy-to-navigate as tig . tig A text-mode interface for git that allows you to easily browse through your commit history. It is not part of git and needs to be installed ( apt install tig for Debian/Ubuntu; Homebrew instructions for macOS) git push Push your local changes to another repository, for example on GitHub. git pull Pull changes from another repository to your local repository. GitHub GitHub is an online platform where you can store and share your projects; it is especially well suited for working on a project with several other people. It acts as a central place where everyone can access/contribute to the project and offers several useful tools (issues, wikis, project milestones, user management, etc) that make collaboration simple and easy. To create a profile, go to GitHub , and from there, follow the prompts to create your account. GitLab GitLab is much like GitHub, but is open source and can be hosted by anyone. Pull Requests Platforms like GitHub and GitLab enable pull requests (called a merge request on GitLab) to propose and collaborate on changes to a repository. A typical pull request workflow looks like this: On the platform of your choice, fork the repository you want to contribute to (commonly called \"upstream\"). This creates a copy of the upstream repository under your user account, to freely make changes without affecting the original project. Clone your fork of the repository on your local machine. Create a new branch in your local clone ( git checkout -b mybranch master ). Make your changes locally and commit them. Push your changes to your fork. On GitHub/GitLab, go to your fork, select the new branch, and create a pull request to the upstream repository. The above works well for your first PR. But what if you worked on other projects for a few months, upstream development has continued, and now you want to propose another change to the current upstream code? Add the upstream repository as a remote on your local clone ( git remote add upstream <url> ). Note, this only has to be done once. Fetch the latest changes from upstream ( git fetch upstream ). Create a new branch in your local clone, based on upstream's master branch ( git checkout -b mybranch upstream/master ). Then proceed as usual, making your local changes, committing, pushing to your fork, and then opening a PR through the web UI. The following diagram can help visualize how the above steps work together. Resources GitHub offers an interactive Git tutorial that is a great starting point for beginners. Atlassian provides a nice overview of both the forking workflow . and Pull Request workflow that are common in our projects and the wider Open Source community. The free Pro Git Book covers just about everything Git has to offer using clear and easy-to-understand language. It starts with the basics, but builds up to some of Git's more complex features. If you like video tutorials, the Intro to Git and GitHub and The Basics of Git and GitHub videos are worth watching to learn about the basics of Git and GitHub and want a step-by-step explanation of how to get started. For any questions you might have about using GitHub, see GitHub Help . The Git Reference Manual is the official docs for Git. It has all the information you could want to know about Git, but is pretty dense and better suited for intermediate and advanced users.","tags":"tools","url":"tools/git/","loc":"tools/git/"},{"title":"HTCondor","text":"HTCondor (also known as condor) is a job scheduler. It is powerful, but conceptually quite simple. Condor: runs the jobs you tell it to finds places to run those jobs makes sure everyone has fair access to resources There are two aspects when teaching people how to use Condor: Using Condor itself (useful commands, the .submit file). This is relatively easy , and is what this document focuses on. Understanding the problems your scripts are trying to solve, and how to break those up into conceptually discrete steps and units. Put another way: how to not use the cluster like one big laptop and instead like thousands of tiny desktops. For many people, this is the not so easy part. A Simple Sample Let's keep things simple. You want to run the command fortune using condor. Just one job. Here's how we do so: create a file called fortune.submit defining your job: # The environment universe = vanilla getenv = True request_cpus = 1 request_memory = 1G # Execution initial_dir = $ENV(HOME) executable = /usr/games/fortune # Job log = $ENV(HOME)/logs/fortune_$(Cluster).$(Process).log output = $ENV(HOME)/logs/fortune_$(Cluster).$(Process).out error = $ENV(HOME)/logs/fortune_$(Cluster).$(Process).err Queue create a folder for the logs: mkdir ~/logs submit the job to condor condor_submit fortune.submit watch the queue (the job will move from IDLE, to RUN, and then then will disappear, meaning it has finished). condor_q read the output file containing your fortune cat ~/logs/fortune_*.out Anatomy of .submit To accomplish more than the simple example above, you'll need to understand the anatomy of a submit file. A .submit file describes the jobs (commands and their arguments) that condor will run, the environment they will run in, and the needed hardware resources (RAM, CPU). This example defines two jobs, and this time with an argument. One job outputs the calendar for 1985 and the second job for 1986. # The environment universe = vanilla getenv = True request_cpus = 1 request_memory = 1G # Execution initial_dir = $ENV(HOME) executable = /usr/bin/ncal # Logs log = $ENV(HOME)/logs/$(Cluster).$(Process).log output = $ENV(HOME)/logs/$(Cluster).$(Process).out error = $ENV(HOME)/logs/$(Cluster).$(Process).err # Job 1 arguments = \"1985\" Queue # Job 2 arguments = \"1986\" Queue Breaking this into sections: universe = vanilla getenv = True The first two lines you likely will never need to change. universe declares the type of Condor environment used, and getenv tells Condor to copy environmental variables from your execution environment to the compute nodes. Unless you know what you're doing, keep these lines unchanged. request_cpus = 1 request_memory = 1G Often people don't know how much RAM their job will consume. In that case, make an educated guess, and then submit a single job. When it completes, check its .log file. It contains information about the memory usage of the job. initial_dir = $ENV(HOME) This is the directory that Condor will cd to when starting your job. All paths are relative to this starting directory (unless they are absolute paths, i.e. starting with a / ). In this case, it is your user's home folder. executable = /usr/bin/ncal Next comes the executable . It is common for users to simply enter the command name. This is often wrong. Re-read the description above for initial_dir , and you will see that if executable is set to ncal , it would try to run /$HOME/ncal . This is usually what you want when you're executing a script you've written, but it's not what you want when executing a system utility. In that case, use an absolute path. # Logs log = $ENV(HOME)/logs/$(Cluster).$(Process).log output = $ENV(HOME)/logs/$(Cluster).$(Process).out error = $ENV(HOME)/logs/$(Cluster).$(Process).err The log files store information about the job. The $(Cluster) and $(Process) macros supply the job ID, and are used here to create unique log files for each job. log : for information about the condor job (duration, memory usage, the machine it ran on, etc) output : anything the job writes to stdout error : anything the job writes to stderr # Job 1 arguments = \"1985\" Queue The arguments are what is passed to the executable . Then comes Queue . This means \"submit a job\". The state of all variables up to this point will be submitted as a job. We will soon see, with the second job, how this is powerful. # Job 2 arguments = \"1986\" Queue The arguments variable is overwritten, and then we Queue another job. It's as simple as that. In this case, jobs 1 and 2 are identical except for their arguments. You may wonder how the log files are unique for each job if we havn't redefined them. This is because we're using condor macros to refer to the job ID. That being said, it is quite common to redefine the log files for each job, containing more human-useful information. Generating a .submit Condor's strength is not running one job at a time. Its strength is running thousands of jobs at a time, and no one in their right mind writes such submit files by hand. A simple script is used to generate them. We'll do a repeat of the above jobs, but this time outputing calendars for the last ~1,000 years. #!/bin/sh # v3.0 logs_dir=~/logs # create the logs dir if it doesn't exist [ ! -d \"$logs_dir\" ] && mkdir -p \"$logs_dir\" # print the .submit header printf \"# The environment universe = vanilla getenv = True request_cpus = 1 request_memory = 1G # Execution initial_dir = \\$ENV(HOME) executable = /usr/bin/ncal \\n\" # create a job for each subject file for year in $(seq 1000 1999); do printf \"arguments = ${year}\\n\" printf \"log = ${logs_dir}/y${year}_\\$(Cluster).\\$(Process).log\\n\" printf \"output = ${logs_dir}/y${year}_\\$(Cluster).\\$(Process).out\\n\" printf \"error = ${logs_dir}/y${year}_\\$(Cluster).\\$(Process).err\\n\" printf \"Queue\\n\\n\" done Let's run the script and make sure that the output looks sane (if it fails with \"permission denied\", you probably forgot to mark it as executable by using chmod +x ). ./ncal_submit_gen.sh If everything looks good, then it's time to submit the jobs directly to condor. ./ncal_submit_gen.sh | condor_submit And you just submitted 1,000 jobs to condor. Interactive Jobs To work interactively on a compute node instead of the head node, there are two ways: run a default interactive job with condor_submit -interactive , i.e. without specifying any submit-file. submitting a job which can specify additional, non-default values. An example submit file for an interactive job is: # The environment universe = vanilla getenv = True # Auto-exit after being idle for 2 hours (7200 seconds) # If you are tempted to set this to a high value: just don't. Jobs sitting # idle block other users from executing jobs. Don't be that person. environment = \"TMOUT=7200\" # Required Resources request_cpus = 1 request_memory = 4G # Execution initial_dir = $ENV(HOME) executable = /bin/bash # Logs log = $ENV(HOME)/logs/$(Cluster).$(Process).log output = $ENV(HOME)/logs/$(Cluster).$(Process).out error = $ENV(HOME)/logs/$(Cluster).$(Process).err # Job - spawn one instance Queue NOTE: An interactive session is blocking the requested CPU(s) and memory for your use, potentially preventing others to run their jobs. If you do not plan to work within the next 1-2 hours using the interactive session, exit it and resubmit a new interactive job later. For long-running processes (e.g. importing DICOMs using datalad-hirni ), you should start this interactive job from a tmux session . That is, you should log into the head node as usual, start tmux and then start the interactive session. This way, the interactive session will be part of your tmux session, which you can detach and re-attach later. Useful Commands List all slots (available and used) and their size condor_status Submit a job/job cluster condor_submit <file.submit> To gain access to an interactive shell on a node — even with a GUI. condor_submit -interactive <file.submit> Summary of your jobs in the queue condor_q All of your running jobs and which machine they are on condor_q -nobatch -run All jobs from all users in the queue condor_q -nobatch -allusers Explain why a job is in a particular state condor_q -better-analyze <jobid> Remove jobs from the queue condor_rm <username> # remove all jobs for this (your) user condor_rm <clusterid> # remove all jobs belonging to this cluster condor_rm <clusterid>.<jobid> # remove this specific job User statistics, priority, and priority factor condor_userprio --allusers For those who are more familiar with Sun's GridEngine, Condor provides condor_qsub . condor_qsub Documentation The official Condor documentation is long, but comprehensive. If you have questions, their docs are a great resource. Pay special attention to the sections on Submitting a Job and Managing a Job .","tags":"tools","url":"tools/htcondor/","loc":"tools/htcondor/"},{"title":"EXFA: Skyra","text":"NOTE: This documentation is in progress. Overview TODO: add contact info Software There is one presentation computer, provided by the biopsych lab. It runs both Windows XP and Debian Jessie. Debian This is the standard experiment machine setup. Matlab (2012b), PsychoPy, and Psychtoolbox are installed. The machine has been configured to match the timing/configuration requirements of both PsychoPy and Psychtoolbox. Windows An ancient Windows XP install. It is, for obvious reasons, deprecated, but it is functional. Presentation is installed. Scanner 3 Telsa Siemens Magnetom Skyra TODO: discuss scanner triggers Projector TODO: Describe and link: projector Peripherals TODO: Describe parallell response box","tags":"labs","url":"labs/exfa_skyra/","loc":"labs/exfa_skyra/"},{"title":"Python","text":"Debian Packages If you're running Debian, it's often easiest to use the official Debian packages (when present) to install python modules. Search for them via apt . For example: apt search mvpa2 Once you've found the package, install it: apt install python-mvpa2 If you can't find a Debian package for the module you want installed, then you can to install it using pip — preferably in a virtual environment (see below). Virtual Environments Virtual environments (AKA \"venvs\") allow you to create isolated environments for Python. These environments still have access to the full filesystem, but their \"Python World\" is their own sandbox to play in, and is completely independent of everything else. The advantages of this isolation are numerous (tidiness, one project's dependencies won't affect others, ease of troubleshooting, etc). Using venvs should be your default mode of operation. Virtual environments can be created anywhere on the filesystem, but I like to keep them all together in a hidden folder called .venvs in my home directory. To create a venv for your new hyperalignment project , run the following: python3 -m venv ~/.venvs/hyperHyper NOTE: Python 2.7 users will need to use the virtualenv command (e.g. virtualenv ~/.venvs/hyperHyper ) To activate your new venv run: source ~/.venvs/hyperHyper/bin/activate Your shell's prompt will change to denote the venv you are in. Now you can install whatever packages you need (using pip3 install ); they will all be stored in ~/.venvs/hyperHyper and will only be available when this venv is activated. When you're done, deactivating is as simple as running: deactivate IPython IPython is an interactive shell to compute Python code, similar to the Bash shell or the Matlab prompt. IPython features tab-completion, command history retrieval across sessions, dynamic object introspection, and magic commands that provide some nice quality-of-life syntactical sugar. To begin an IPython session, simply run: ipython Resources Interactive Book: Foundations of Python Programming A \"projects first\" approach that focuses on building things using Python rather than focusing on the language itself. Book: Learn Python the Hard Way Popular with good content. People either enjoy or strongly dislike the book's approach of typing out every exercise, embracing failure, and working through problems. Book: Python Crash Course A good go-to book for learning Python. Website: The Python Tutorial The official tutorial from the Python Project.","tags":"tools","url":"tools/python/","loc":"tools/python/"},{"title":"R","text":"Debian Packages If you're running Debian, it's easiest to use the official Debian packages (when present) to install R packages. Search for them via apt . For example: apt search tidyr Once you've found the package, install it: apt install r-cran-tidyr If you can't find a Debian package for the R module you want installed, then you can install it directly from CRAN (see below). CRAN CRAN is the main repository for community projects. Installing packages from CRAN can range from dull, to thrilling, to aggravating beyond all belief. It will quickly become apparent which adventure you have chosen. It is best to install packages to your home folder; this avoids the need for admin privileges. First create a directory for the packages: mkdir -p ~/.R/library Then tell R you want to use this folder: echo 'R_LIBS_USER=\"~/.R/library\"' > ~/.Renviron Packages from CRAN can now be installed using ìnstall.packages()` and will be installed into ~/.R/library . For example, to install packrat, start R and then run: > install.packages(\"packrat\") Packrat Packrat allows you to create isolated environments for R projects, similar to Python's \"virtualenv\". It is not installed by default, so you will need to follow the above instructions to install it. To use packrat for your new stats project called \"probably\", we first need to create the project folder: mkdir -p ~/.renvs/probably Now, start R and initialize the folder as a packrat project: > packrat::init(\"~/.renvs/probably\") Now you can install whatever packages you need (using install.packages() ); they will all be stored in ~/.renvs/probably and will only be available when this packrat instance is activated. When you're done, deactivate it: > packrat::off() If you start R from the project's packrat folder ( ~/.renvs/probably in this case), packrat will start the environment automatically. Resources Book: Statistical Rethinking Bayesian statistics and general intellectual superiority. (english) Book: Discovering Statistics Using R A good, standard book with a narrative. (english) Book: Grundlagen der Datenanalyse mit R Encyclopedia style, covering all standard statistics. (german)","tags":"tools","url":"tools/r/","loc":"tools/r/"},{"title":"sixel","text":"Sixel allows displaying graphics at the command line; it's technology from the 80s that never made it — until quite recently. A picture is worth a thousand words: Utilities img2sixel niicat (not yet installed on Medusa; users should install in a venv for now) Setup Currently, very few terminals support sixel. macOS Download and install iTerm2 . The built-in terminal does not support sixel. Linux xterm is the recommended terminal for using sixel. None of the following popular terminals have in-tree support: GNOME Terminal, Konsole, urxvt, st. If you're using the IPSY dotfiles then everything should \"just work\". Otherwise, you will need to adjust your .Xresources settings. Testing A test image is provided as part of the IPSY dotfiles : cat ~/.dotfiles/test/snake.six","tags":"tools","url":"tools/sixel/","loc":"tools/sixel/"},{"title":"SSH","text":"SSH is used to securely login to a machine over the network. It is most commonly used to start an interactive shell on the remote server, but many commands (such as rsync and scp ) are capable of using the SSH protocol to securely transfer bulk data. SSH comes installed by default on all modern machines (Linux, macOS, and Windows 10). Using SSH Logging into a remote machine is quite simple with SSH: ssh username@medusa.ovgu.de If it is the first time that you have connected to this server with SSH, you will receive a prompt similar to the following: The authenticity of host 'medusa.ovgu.de (141.44.17.54)' can't be established. ECDSA key fingerprint is SHA256:fgvO3iB0fUsVjbrXLhg8h8ZPZdXa55rnwR+9P72O7oU. Are you sure you want to continue connecting (yes/no/[fingerprint])? Type \"yes\" to confirm the fingerprint (please see the explanation of fingerprints below). Authenticate, and you now have an interactive session on the remote machine. Keys SSH also supports key-based authentication, which is much more secure than password authentication. It is a common misconception that using keys means that you don't need a password. Though a key instead of a password is used to authenticate to the server, a password is used to protect the key itself on your system (it is possible to create keys that are not password protected, but it is considered poor practice). Protecting your key with a password is important. Otherwise, if someone accesses your computer, they will gain access to every system that uses that key. That's bad (technical term). If you don't yet have an SSH key, generate one by running the following, and follow the prompts: ssh-keygen -t rsa -b 4096 Once you have a key, it can be copied to a server using ssh-copy-id . For example: ssh-copy-id username@medusa.ovgu.de X Forwarding Graphical programs run on the remote server can be displayed on your local machine using a feature called X Forwarding. To enable this, use the -X flag. For example: ssh -X username@medusa.ovgu.de However, X Forwarding comes with one major caveat: it is very sensitive to latency, so it is only practical to use when on the same wired network as the server (i.e. on campus). X Forwarding also requires that your local machine has X installed. This is the default on Linux systems, but macOS systems need to install XQuartz [1] while Windows 10 systems need VcXsrv installed via WSL. Jump Hosts If you want to connect to a server that is on a private network, you will want to use a Jump Host. To explain, let's say we're robbing a bank (sometimes called \"self financing\" in academia). The server vault isn't publicly available, but lobby is, so we're going to connect to it first and use it as a jump host. For example: ssh -J username@lobby username@vault This will first connect to lobby , and then connect to vault . This is more convenient than manually SSHing ( ssh lobby followed with ssh vault ), but more importantantly , it allows for the SSH keys from the original computer to be used to authenticate to both servers. Otherwise, you would need to store a copy of your keys on lobby , which is unsafe. They're called keys for a reason: keep them secret; keep them safe. Agents An agent (e.g. ssh-agent ) can be used to remember the password to unlock your key, usually with a timeout. This can be quite convenient when connecting frequently to servers. TODO: Discuss agents. ssh-agent is most often suggested, but its behavior is non-obvious, especially when compared to gpg-agent (which is an option when using RSA keys). On macOS, Apple keychain can be used. Fingerprints Each server has a unique fingerprint that identifies it. In theory, when first connecting to a server, users should take great care to verify that the fingerprint offered is authentic by confirming it against a trusted copy via a different (preferably analog) communication channel (e.g. phone). This allows you to be certain that you are indeed connecting to the server that you think you are, and that no one is attempting a Man-in-the-Middle attack . In reality (and unsurprisingly), the majority of users do not perform such steps. However, fingerprints still have value, because SSH will notify you if the server's fingerprint changes. This helps protect against future MITM attacks. [2] Your SSH client maintains a list of server fingerprints in the ~/.ssh/known_hosts file. Config SSH has many powerful options. If you want to use different keys for different hosts, use jump hosts automatically when connecting to certain hosts, etc, then you should read man ssh_config . These options are all set in the ~/.ssh/config file. [1] There have been some problems with recent XQuartz releases, but users have reported that version 2.7.7 works best for them. It is recommended to use that version until 2.7.12 is released. [2] As always, there are many details and nuances that make this technically untrue. But it is a reasonable approximation for what a user's understanding of the situation should be.","tags":"tools","url":"tools/ssh/","loc":"tools/ssh/"},{"title":"tmux","text":"tmux is a tool to manage terminal sessions. For those familiar with screen , tmux is a more featureful and better maintained alternative. The primary functionality discussed here is tmux's ability to detach and re-attach sessions, without affecting the programs running within the session. When using unstable network connections (such as bad wifi), this can save you significant frustration. If the VPN or SSH connection drops, you can just reconnect and reattach the tmux session without losing any of your work. tmux has many other useful features, including displaying multiple terminals at once by splitting the window into panes. In addition to the man page ( man tmux ), the The Tao of Tmux (especially chapters 7 and 11) is a popular reference. The Linux Academy also offers a convenient tmux cheat sheet , to help with frequently used tmux commands and keybindings. Example As a practical example, let's run tmux on medusa : aqw@medusa:~$ tmux And to celebrate creating our first tmux session, let's run cmatrix : cmatrix That's one Keanu \"whoa\". Our terminal is now state-of-the-art 1999. Sessions in tmux can be detached and reattached, without interrupting the program running inside. To detach, type ctrl + b , and then type d . Your session is detached, and you're back at the prompt. When we reattach, cmatrix will still be running, right where we left it: tmux attach Let's detach again from our current session ( ctrl + b then d ), and create another session. This one we'll name, for convenience (and style): tmux new -s inigo_montoya And we're in a new tmux session, this one named \"inigo_montoya\". Let's detach (you should be getting good at this now). At the prompt, list the running tmux sessions: aqw@medusa:~$ tmux list-sessions 0: 1 windows (created Sun Apr 28 13:09:06 2019) [119x39] inigo_montoya: 1 windows (created Sun Apr 28 14:00:29 2019) [119x39] Session 0 is the first tmux session we created (with cmatrix running in it). inigo_montoya is the session we just created. To attach a specific session, run: tmux attach -t inigo_montoya Sessions can be quit either by typing exit from within the session, or with kill-session : tmux kill-session -t 0 tmux kill-session -t inigo_montoya","tags":"tools","url":"tools/tmux/","loc":"tools/tmux/"},{"title":"Data Center","text":"We have 5U in one of the racks in the G26 data center. It houses the backup server (Thunk). PWR U# Name Inventory # Overview 4 JBOD (Thunk) (likely part of 264097,000) 9x 4TB drives / 12x bays 5 6 Thunk 261309,000 2x 6-core 2.4 GHz Xeon E5645 96 GiB RAM 12x 4TB drives / 16x bays 7 8 In the G01 data center, we own a 42U rack. The contents of the rack are (as of 2018.08.09): PWR U# Name Inventory # Overview N/C 1 UPS 243181,000 1000 VA 2 3 Router 1019378 SG-8860 4 DMZ Switch 48 Port HP V1910-48G 5 Cable Management 6 Mgmt. Switch 24 Port HP ProCurve 1700-24 7 Cable Management 8 Data Switch 1019379 28 Port Netgear XS728T 9 10 Mulder 260876,000 1x 4-core 3.2 GHz Xeon E3-1230 16 GiB RAM 11 12 13 14 15 16 17 18 19 20 21 N/I 22 Sunshine 252757,000 Sun Fire T2000 1x 8-core 1.2GHz 16 GiB RAM 23 24 25 snake11 1019380,000 2x 10-core 2.3 GHz Xeon E5-2650v3 96 GiB DDR4 RAM 26 27 snake10 267616,000 4x 8-core 2.8 GHz Opteron 6320 512 GiB RAM 28 snake9 246831,000 1x 8-core 2.0 GHz Opteron 6128 64 GiB RAM 29 snake8 245075,000 1x 4-core 2.67 GHz i7 920 18 GiB RAM 30 31 snake7 4x 16-core 2.4 GHz Opteron 6272 256 GiB RAM 32 snake5 & snake6 261309,000 (guessing) each snake: 2x 6-core 2.4 GHz Xeon E5645 96 GiB RAM 33 snake3 & snake4 34 snake1 & snake2 PDU & UPS 35 Medusa 265021,000 4x 8-core 2.8 GHz Opteron 6320 256 GiB RAM 36 PDU & UPS 37 Zing 1031333 13x 3.84TiB SanDisk Optimus 2 Max / 24x bays 38 PDU & UPS 39 JBOD (Zing) 265526,000 6x 4TB HDD / 12x bays 40 41 UPS 3000 VA 42 The rack's Inventory Nr.: 248252,00 Legend: N/I: not installed (physically not in the rack) N/C: not connected (physically in the rack)","tags":"medusa","url":"medusa/data_center/","loc":"medusa/data_center/"},{"title":"Hardware","text":"Summary As of July 2020, the cluster comprises 15 nodes with over 250 CPU cores and 2.25 TiB of RAM. Centralized storage features more than 40 TiB of high performance SSD and 11 TiB of HDD capacity, and is accessed by cluster nodes via 10Gb Ethernet. Head Node (Medusa) 4x 8-core 2.8 GHz Opteron 6320 256 GiB RAM (16x 16GiB DDR3 ECC reg) 1x 10Gb NIC Purchased 2013.12. Supermicro's specs: A+ Server 2042G-TRF . Data Node (Zing) 1x 8-core 3.2 GHz Xeon E5-1660 v4 96 GiB RAM (6x 16GB DDR4 2133 ECC reg) 42 TiB SSD (~29.3 TiB usable) and ~26 TiB HDD (~10.5 TiB usable) storage 2x 10Gb bonded NICs Purchased in 2016.12. Supermicro's specs: SuperChassis 216BE1C-R920LPB and Mainboard X10SRL-F Backup Node (Thunk) The only server in this list that is not hosted in G01. It is instead across campus in the G26 data center. 2x 6-core 2.4 GHz Xeon E5645 96 GiB RAM (12x 8GB DDR3 ECC reg) 76.4 TiB HDD (52.1 TiB usable) storage 1x 1Gb NIC Purchased 2011.12. Supermicro's specs: SuperChassis 836E16-R1200B and Mainboard X8DTH-iF snake1-6 2x 6-core 2.4 GHz Xeon E5645 96 GiB RAM (12x 8 GiB DDR3 ECC reg) 1x 1Gb NIC Purchased 2011.12. Supermicro's specs: Twinserver 6016TT-TF snake7 4x 16-core 2.4 GHz Opteron 6272 256 GiB RAM (32x 8 GiB DDR3 ECC reg) 1x 1Gb NIC Purchased 2012.06. Supermicro's specs: H8QG6+-F Motherboard snake8 1x 4-core 2.67 GHz i7 920 18 GiB RAM 1x 1Gb NIC Purchased 2010.03; formerly amras. Supermicro's specs: RM21706 Chassis and X8STE Motherboard snake9 1x 8-core 2.0 GHz Opteron 6128 64 GiB RAM (8x 8GiB DDR3 ECC Reg) 1x 1Gb NIC Purchased 2010 (estimated); formerly \"just laying about\" in Toemme's lab. Supermicro's specs: H8DGU-F Motherboard snake10 4x 8-core 2.8 GHz Opteron 6320 512 GiB RAM (32x 16GiB DDR3 ECC Reg) 1x 1Gb NIC Purchased 2013.12. Supermicro's specs: A+ Server 1042G-TF snake11 2x 10-core 2.3 GHz Intel Xeon E5-2650v3 96 GiB RAM (6x 16GiB DDR4 ECC Reg) 1.2 TB NVMe mounted at /tmp 1x 10Gb NIC Purchased 2015.12. Supermicro's specs: 825TQ-R740LPB Chassis and X10DRi-T Motherboard Mulder TODO: Add Mulder's specs Networking An SG-8860 running OpenBSD acts as the router/firewall/etc for the cluster. All cluster traffic uses one 1Gb connection. All web services use a separate 1Gb connection. Internally, the data network is 10Gb (though not all hosts have 10Gb cards). Both the management and DMZ networks are physically separate. Power We have one zero-U 3-phase PDU ( Raritan PX2-2730 ). It is connected to a red IEC_60309 power plug. For specifics on which machine is plugged into which PDU outlet, consult the rack diagram . As we have limited battery capacity, only critical equipment is protected by the UPSs. Both UPSs are monitored by zing via NUT ; the head node and zing poll this information; if the main UPS (Eaton) reaches low battery , both Medusa and Zing will shutdown immediately. APC Smart-UPS SC 1000 age: ~2009 protects router and switches Eaton 5PX 2200 age: 2012.12 protects Medusa and Zing","tags":"medusa","url":"medusa/hardware/","loc":"medusa/hardware/"},{"title":"Services","text":"In addition to the computational cluster and experimental labs , IPSY and the wider OvGU community provide a range of services.","tags":"pages","url":"services/","loc":"services/"},{"title":"Communication","text":"These are the primary methods of communication in the lab: OvGU Email Once your account is setup, you can access your email via many ways: Webmail via email client of your choice. A detailed description on the configuration can be found here Mailing Lists exppsy@ovgu.de Subscribe to exppsy neuropsy-list@ovgu.de Subscribe to neuropsy biopsy-l@ovgu.de Subscribe to biopsy brazi-l@ovgu An IPSY-wide mailing list that emails directly to the above three lists, plus members of other IPSY labs which don't have a mailing list. Subscribe to brazi-l Mattermost The faculty for computer science hosts a mattermost instance to which you can register and join an existing team. For that a team admin has to invite you to it. Team admins currently are: Nico Marek , Peter Varvra and Manuela Kuhn Every team has two types of channels: public: everybody can join private: one can only join after an invitation of a channel member By default every team has two public channels: \"Off-Topic\" and \"Town Square\". In addition one-to-one-communication between people is also possible via \"direct message\". A more detailed explanation on how to use mattermost can be found on the official mattermost documentation .","tags":"services","url":"services/communication/","loc":"services/communication/"},{"title":"Hosted","text":"Webspace Web servers are available for: Lab/Project Websites such as studyforrest.org , howdoyouscience.net , etc Data Distribution such as psydata.ovgu.de Personal Websites Every IPSY user has a public_html/ folder in their home directory on kumo.ovgu.de . Any files placed in that folder will be made available at http://kumo.ovgu.de/~<username>/ . DNS Some projects have their own domain names, but the majority of systems will live under the following namespaces. ipsymd.de For convenience and brevity. As this name is new, only a few services use it, but in the end, most internal services will migrate to it. ovgu.de For official, outward facing services that should display the full OvGU branding (website, major projects, etc). Debian Repository IPSY-specific packages (scripts, configuration) and license restricted software (Freesurfer, Matlab, etc) is available. To add the repo to your Debian machine, run the following: printf \"deb http://kumo.ovgu.de/debian stretch main\\n\" | sudo tee /etc/apt/sources.list.d/ipsy.sources.list wget -O- http://kumo.ovgu.de/debian/ipsy_apt.gpg.key | sudo apt-key add - sudo apt update sudo apt search ipsy- Due to the restrictive licensing, this repository is only available to machines with a wired connection in the IPSY offices or via VPN. Dotfiles An IPSY dotfiles repository is provided ( ssh://medusa.ovgu.de:/home/git/dotfiles.git ), containing a collection of config files with sane defaults to enable a baseline, reasonably featured command line experience. The README file in the repository contains instructions on how to use dotfiles. JupyterHub (retired) The IPSY JupyterHub instance is now retired. The URZ, as a result of our successful trial, will host a JupyterHub instance and make it available for university use sometime in 2020. TODO: Add link/contact info here for the URZ instance. OwnCloud (EOL) The OwnCloud deployment is now deprecated and no new users are permitted. Existing users are being migrated to other solutions.","tags":"services","url":"services/hosted/","loc":"services/hosted/"},{"title":"Network","text":"Wired Network Most network jacks are activated. If you need network access and the jack isn't active, contact Nico Marek and he will ask for it to be enabled. Wireless Network There are many WiFi networks available on campus. We recommend the following networks: eduroam Uses your OvGU email address and password to authenticate. Can be used at any institution participating in the eduroam network . OvGU-802.1X Uses your OvGU account. Is considered part of the OvGU network, and can access intranet resources. VPN VPN access allows you to remotely connect to resources that are only available from within the OvGU network (such as licensing servers, journal access, etc). VPN Type: Cisco IPSec host: vpn.ovgu.de user: <your ovgu account> (same one to authenticate for email) password: <your password> group-name: vpn1 shared secret: vpn1 In general, it is easiest to use your OS's built-in VPN support. However, Cisco does provide a VPN client (Cisco AnyConnect), and the URZ provides detailed instructions on how to configure it.","tags":"services","url":"services/network/","loc":"services/network/"},{"title":"Printing","text":"When in doubt, use the drivers bundled with your OS. Otherwise, the links below will help you find the appropriate drivers. Ullsperger Lab Kyocera FS-C5400DN 141.44.98.7 Room G24-003 Pollmann Lab Kyocera ECOSYS P6035CDN 141.44.98.9 Room G24-010 Joachim Lab Kyocera FS-C5250DN 141.44.98.12 Room G22A-329 005 MFP Brother MFC-9450CDN 141.44.98.11 Room G24-005 005 Color Kyocera FS-C5150CDN 141.44.98.13 Room G24-005 Junior XXIII Kyocera FS-C5150CDN 141.44.98.14 G23 Kitchen G24 Copier/Printer e-STUDIO 256SE 141.44.96.89 Main floor (near vending machines)","tags":"services","url":"services/printing/","loc":"services/printing/"},{"title":"Frequently Asked Questions","text":"Backup Policy For the cluster and all servers, backups are performed daily . For laptops and desktops, there is no centralized backup process. It is your responsibility to come up with a solution that meets your needs. As examples, some users: backup their machines to an external hard drive (e.g. Time Machine) sync with DropBox/Google Drive simply don't keep important data on their laptops Software Licensing Martin Krippl is responsible for IPSY proprietary software licensing. He's the one to talk to if you need Office, EndNote, Matlab, SPSS, etc. Hardware Recommendations Nico Marek can help you if you have questions about the fit of a given hardware solution or need help coming up with the specs for a new hardware purchase. Once determined, you will need to work with the secretary responsible for your lab. They can best guide you through the purchasing process and inform you of any strings that may be attached with the available funds. Updating Website Content The websites for IPSY and most of its labs use OvGU's official Content Management System. Information about how to update content in their system can be found at cms.ovgu.de . If the content you want to add doesn't need to be on the main website, then perhaps your personal webspace on kumo would be a good fit. Transferring Data There are numerous methods to share/publish/exchange data with others. \"Small\" (< 5 GB) Use your personal webspace on Kumo. Larger, One-time Transfers A dedicated account ( ferry ) is used on Medusa to exchange larger collections of data with non-Medusa users. rsync is highly recommended. If you need this setup, ask Nico Marek . Publishing Datasets Many large datasets are available for public consumption on psydata.ovgu.de (AKA Mulder). Data are published both via an rsync daemon and on a web server. http://psydata.ovgu.de","tags":"pages","url":"faq/","loc":"faq/"},{"title":"Contributing to the Docs","text":"Found a problem or have a suggestion for how these docs can be improved? You can report it on the issues tracker on GitHub. And... while bug reports are welcome, patches are even more welcome. ;-) The git repository for this site is hosted on GitHub . If you are not already familiar with git and/or GitHub, read our git documentation first. Content is written in reStructuredText and the site is generated by Pelican . To setup the build environment: clone the repo from GitHub and pull submodules: git clone https://github.com/psychoinformatics-de/ipsy-docs.git cd ipsy-docs git submodule update --init --recursive setup and activate a virtualenv : python3 -m venv ~/.venvs/pelican source ~/.venvs/pelican/bin/activate install Pelican and dependencies: pip3 install pelican beautifulsoup4 run the development server: make devserver point your browser to http://127.0.0.1:8000 . Your copy of the docs will be served locally at that address, and any changes will automatically trigger a rebuild. If you are not familiar with the process of git add -p , git commit , and git push , then now is a good reminder for you to read the git documentation in order to push your changes.","tags":"pages","url":"contributing/","loc":"contributing/"},{"title":"News","text":"Known Issues \"Some\" .hdf5 files remain locked after initial creation. The source of this problem is elusive, and makes little sense. The current work-around is to cp the file and then mv the copy over the original. This forces the file to be assigned a new inode, which invalidates the in-file locking. Madness. Events 2020.07.02 - snake12 retired The motherboard of snake12 died. Given the age of snake12 (purchased 2012), it has been decided to retire the machine. It lived a good life, and computed many scientifically relevant findings. 2020.02.05 - Major update to the docs The majority of content from the INM-7 Docs has been ported over and adapted for IPSY. This represents over a year's worth of documentation fixes and improvements. 2020.02.04 - Dotfiles updated The Dotfiles repo has been been entirely overhauled, porting fixes and improvements from the INM-7 dotfiles. It is located at /home/git/dotfiles.git on Medusa. 2019.11.10 - Degraded data array on Medusa Zing (the data node) currently has a bad SSD drive in one of its arrays. The rebuild completed successfully with no data loss. 2019.08.01 - snake12 down snake12 has a hardware failure and needs to be repaired. 2018.12.17 - ZFS upgrade and reboot; IPython history functional again The data node's version of ZFS was upgraded to 0.7.12. This brings a wide range of fixes. The cluster was rebooted for the update to take effect. As a result of the reboot, the IPython history problem was cleared, though unfortunately not fully understood. 2018.12.14 - SPSS 25 SPSS 25 has been packaged for Debian. The university's SPSS 24 license has expired, so to continue using SPSS, you must upgrade to this new package. To upgrade, simply run apt-get update and apt-get install ipsy-spss25 . 2018.08.31 - ZFS upgrade The data node's version of ZFS was upgraded to 0.7.9. This brings a wide range of fixes. The cluster was rebooted for the update to take effect. 2018.08.02 - Wine 3.0.1 installed cluster-wide If you really need to shoe-horn your Windows-based workflow onto our Debian cluster, then there is a small ray of hope for you. If you can get your application to run via Wine 3.0.x, you can now run it across the entire cluster. 2018.07.09 - Increased number of Matlab toolbox licenses The university has a new licensing agreement with Mathworks. In all practicality, there are now an unlimited number of Matlab and toolbox licenses (10,000). Thus, Matlab users on Condor no longer need to limit the number of compute nodes used due to licensing constraints. The Condor/Matlab documentation has been updated to reflect this. 2018.07.09 - Fixed swap vs /tmp disk allocation on compute nodes Due to a bug in the installation's preseed configuration, compute nodes with large hard drives were allocating the excess space to the swap partition rather than the /tmp directory. This has been fixed, and all nodes have been reinstalled. It is now possible for jobs to run which need a large amount of local disk space rather than NFS. 2018.07.06 - Updated nibabel, nipype, indexed-gzip, fsleyes Updated version of these packages have been installed, which should finally allow them all to coexist in fully updated harmony. Previously, many tools were displaying warnings, and a downgraded version of nibabel was needed to keep everything functional. 2018.06.13 - Signing Key for IPSY's Debian Repo Expired The signing key for IPSY repository of Debian packages on Kumo expired. It has been updated and the updated key deployed to all cluster systems. If this is affecting you on your local system, run the following: curl http://kumo.ovgu.de/debian/ipsy_apt.gpg.key | sudo apt-key add - sudo apt-get update sudo apt-get install --only-upgrade ipsy-keyring If prompted about a conflicting ipsy.gpg file, respond with Y . 2018.04.11 - Condor jobs fail to start on snake4 When jobs attempted to run on snake4, they would bounce between running and idle and complain in the logs about a \"Shadow Exception\". The cause was a deeply mangled /etc/passwd file. The node has been reinstalled. 2018.04.05 - fsleyes crashes on start An updated dependency of fsleyes caused it to crash. The bug was reported, the upstream maintainer released a fix, and that fix has now been deployed. 2018.03.16 - DataLad Upgrade DataLad was upgraded and moved from a system package to a singularity container. Most users shouldn't notice a difference, but if you were using any of its Python libraries directly, they are no longer installed system-wide. 2018.03.13 - HeuDiConv/Nipype Fixed Nipype was failing (prematurely), complaining about an outdated version of Pydot. Until the real fix is applied, an updated version of Pydot has been backported, which seems to resolve the problem. 2018.03.11 - ZFS upgrade The data node's version of ZFS was upgraded to 0.7.6. This brings a wide range of fixes, especially performance related. Hopefully this will end the elusive \"some files, take 1+ minute each to delete\" problem. The cluster was rebooted for the update to take effect.","tags":"pages","url":"news/","loc":"news/"}]};